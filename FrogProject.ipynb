{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchaudio import transforms\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import math\n",
    "input_dir = Path.cwd()\n",
    "files = list(input_dir.rglob(\"*.mp3*\")) # store a list of all audio paths\n",
    "testingPaths = list(input_dir.rglob(\"*.wav*\"))\n",
    "#print(files[0])\n",
    "folders = list(input_dir.glob(\"*class*\"))\n",
    "#print(folders)\n",
    "#RECHANNEL ADDITIONAL DATA, ADD THE FUNCTION]\n",
    "\n",
    "# 7, 65,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = 0\n",
    "# counter1 = -1\n",
    "# finalLabels = []\n",
    "# for i in range(len(files)):\n",
    "#     counter1 = counter1 + 1\n",
    "#     if counter1 % 40 == 0 and counter1 != 0:\n",
    "#         label = label + 1\n",
    "#     print(i, counter1, label)\n",
    "#     finalLabels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(testingPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalLabels[1999\n",
    "                #  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatedFiles = []\n",
    "# num = 0\n",
    "# for i in range(50):\n",
    "#     for j in range(30):\n",
    "#         updatedFiles.append(files[j + num])\n",
    "#     num  = num + 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatedTestingFiles = []\n",
    "# num = 30\n",
    "# for i in range(50):\n",
    "#         for j in range(10):\n",
    "#             updatedTestingFiles.append(files[j + num])\n",
    "#         num = num + 40\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatedTrainingLabels = []\n",
    "# num = 0\n",
    "# for i in range(50):\n",
    "#     for j in range(30):\n",
    "#         updatedTrainingLabels.append(finalLabels[j + num])\n",
    "#     num  = num + 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatedTestingLabels = []\n",
    "# num = 30\n",
    "# for i in range(50):\n",
    "#         for j in range(10):\n",
    "#             updatedTestingLabels.append(finalLabels[j + num])\n",
    "#         num = num + 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(updatedTestingLabels[6])\n",
    "# print(updatedTestingFiles[20])\n",
    "# print(len(updatedTestingFiles))\n",
    "# print(len(updatedTestingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedRecordings = []\n",
    "counter = 0\n",
    "for testingPath in testingPaths:\n",
    "    \n",
    "    \n",
    "    if int(str(testingPath)[55:56]) == 7 or int(str(testingPath)[55:56]) == 6 or int(str(testingPath)[55:56]) == 2:\n",
    "        \n",
    "        if(int(str(testingPath)[55:56]) == 2 ):\n",
    "            \n",
    "            if(str(testingPath)[73:74] == \"G\"):\n",
    "                \n",
    "                 mixedRecordings.append(testingPath)\n",
    "        else:\n",
    "           \n",
    "            mixedRecordings.append(testingPath)\n",
    "        \n",
    "mixedRecordings\n",
    "for recording in mixedRecordings:\n",
    "    for path in testingPaths:\n",
    "        if recording == path:\n",
    "            testingPaths.remove(path)\n",
    "print(len(mixedRecordings))\n",
    "# #21, 65, 7 \n",
    "#0,15,30, 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(mixedRecordings))\n",
    "#mixedRecordings = updatedTestingFiles\n",
    "# print(testingPaths)\n",
    "print(len(mixedRecordings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IDENTIFIER = 57\n",
    "data = [[]]\n",
    "for file, folder in zip(files, folders):\n",
    "    data.append([file, folder])\n",
    "data.pop(0)\n",
    "updatedData = data\n",
    "for i in range(len(updatedData)):\n",
    "    updatedData[i][1] = str(data[i][1])[55:]\n",
    "dftrain = pd.DataFrame(updatedData, columns = ['FilePath', 'Class ID'])\n",
    "dftrain\n",
    "#arranged the meta data with files paths corresponding to classIds\n",
    "print(str(testingPaths[0])[:57])\n",
    "print(str(files[0])[:60])\n",
    "print(testingPaths.index(testingPaths[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = []\n",
    "# morePaths = dftrain['FilePath']\n",
    "# classIds = dftrain[\"Class ID\"]\n",
    "# dict = {}\n",
    "# for i in range(71):\n",
    "#     dict[int(classIds[i])] = str(morePaths[i])[57:-4]\n",
    "# myKeys = list(dict.keys())\n",
    "# myKeys.sort()\n",
    "# sorted_dict = {i: dict[i] for i in myKeys}\n",
    "# sorted_dict\n",
    "# class_names = dict.values()\n",
    "# class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLabels = [] #extends the labels and testingPaths to the data array and label array\n",
    "paths1 = dftrain['FilePath']\n",
    "paths1 = list(paths1)\n",
    "\n",
    "for path in testingPaths:\n",
    "    for nextPath in paths1:\n",
    "      \n",
    "        #print(str(path)[:PATH_IDENTIFIER]  str(nextPath)[:PATH_IDENTIFIER])\n",
    "        if str(path)[:PATH_IDENTIFIER] == str(nextPath)[:PATH_IDENTIFIER]:\n",
    "            \n",
    "            testingLabels.append(int(dftrain.iloc[paths1.index(nextPath)][1]))\n",
    "            break\n",
    "print(dftrain)\n",
    "# print(testingLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = []\n",
    "# for i in range(71):\n",
    "#     class_names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_CLASSES = -1\n",
    "# classes = dftrain[\"Class ID\"]\n",
    "\n",
    "# for aClass in classes:\n",
    "#     if int(aClass) > NUM_CLASSES:\n",
    "#         NUM_CLASSES = int(aClass)\n",
    "# NUM_CLASSES = NUM_CLASSES + 1\n",
    "# print(NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(paths1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths1.append(testingPaths[4])\n",
    "paths1.append(testingPaths[5])\n",
    "paths1.append(testingPaths[7])\n",
    "testingPaths.pop(4)\n",
    "testingPaths.pop(4)\n",
    "testingPaths.pop(5)\n",
    "testingLabels.pop(4)\n",
    "testingLabels.pop(4)\n",
    "testingLabels.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    testingLabels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 3, 3, 4]\n",
    "    print(testingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(updatedFiles)\n",
    "mixedRecordings = testingPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mixedRecordings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths1 = list(paths1)\n",
    "#paths.extend(testingPaths)\n",
    "INITIAL_SAMPLE_NUM = len(paths1)\n",
    "paths1.extend(mixedRecordings)\n",
    "data1 = []\n",
    "for path in paths1:\n",
    "    data1.append(torchaudio.load(path))\n",
    "waveform, sample_rate = torchaudio.load(dftrain['FilePath'][0])\n",
    "#len(data1[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_waveform(waveform, sample_rate):\n",
    "#    # waveform = waveform.numpy()\n",
    "\n",
    "#     num_channels, num_frames = waveform.shape\n",
    "#     time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "#     figure, axes = plt.subplots(num_channels, 1)\n",
    "#     if num_channels == 1:\n",
    "#         axes = [axes]\n",
    "#     for c in range(num_channels):\n",
    "#         axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "#         axes[c].grid(True)\n",
    "#         if num_channels > 1:\n",
    "#             axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "#     figure.suptitle(\"waveform\")\n",
    "#     plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def re_channel(aud):\n",
    "    sig, sr = aud \n",
    "    if(sig.shape[0] == 1):\n",
    "        print(\"hi\")\n",
    "        resig = torch.cat([sig, sig])\n",
    "        return (resig, sr)\n",
    "    else:\n",
    "        return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#print(data1[71][0].shape)\n",
    "sample_data = librosa.load(paths1[3\n",
    "                                 ])\n",
    "\n",
    "#print(sample_data[].shape)\n",
    "spect = librosa.feature.melspectrogram(y=np.array(sample_data[0]), sr=sample_data[1], n_mels=128,\n",
    "                                    fmax=10000)\n",
    "print(spect.shape)\n",
    "S_dB = librosa.power_to_db(spect, ref=np.max)\n",
    "print(S_dB.shape)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=48000,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data1)):\n",
    "    data1[i]  = re_channel(data1[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return (resig, newsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data1)):\n",
    "    data1[i]= resample(data1[i], 48000)\n",
    "\n",
    "for i in range(0, len(data1)):\n",
    "    print(data1[i][0].shape)\n",
    "    print(data1[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1[0][0].shape)\n",
    "# does padding 0s affect output\n",
    "maximum = 0\n",
    "for data in data1:\n",
    "    maximum =  max(data[0].shape[1], maximum)\n",
    "maximum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      print(\"hill\")\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      print(\"hi\")\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      #pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len\n",
    "\n",
    "      # Pad with 0s\n",
    "     # pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat(( sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0, len(data1)):\n",
    "    print(data1[i][0].shape)\n",
    "    print(data1[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data1)):\n",
    "    data1[i]= pad_trunc(data1[i], 5000)# amount of time each clip is\n",
    "    # ELIF STATEMENT IS BROKEN FOR SOME REASON\n",
    "    # why is it broken???\n",
    "var = 1999935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data1)):\n",
    "    print(data1[i][0].shape)\n",
    "    print(data1[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data1[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove talking from recordings\n",
    "\n",
    "# to_remove = 48000 * 8\n",
    "# length = 48000 * 30\n",
    "# data3 = []\n",
    "# to_removeOther = 48000 * 22\n",
    "# #data3 = np.array(data3)\n",
    "# for i in range(0, len(data1)- len(mixedRecordings)):# - len(testingPaths) \n",
    "#     samples, rates = data1[i]\n",
    "#     lst1= samples[0].numpy()\n",
    "#     lst2= samples[1].numpy()\n",
    "#     lst1 = lst1[to_remove:]\n",
    "#     lst2 = lst2[to_remove:]\n",
    "#     lst = [lst1, lst2]\n",
    "#     lst = np.array(lst) #makes it much faster\n",
    "#     lst = torch.tensor(lst)\n",
    "#     data3.append((lst, 48000))\n",
    "# for i in range(len(data1) - len(mixedRecordings), len(data1)):# - len(testingPaths) \n",
    "#     samples, rates = data1[i]\n",
    "#     lst1= samples[0].numpy()\n",
    "#     lst2= samples[1].numpy()\n",
    "#     lst1 = lst1[: to_removeOther]\n",
    "#     lst2 = lst2[: to_removeOther]\n",
    "#     lst = [lst1, lst2]\n",
    "#     lst = np.array(lst) #makes it much faster\n",
    "#     lst = torch.tensor(lst)\n",
    "#     data3.append((lst, 48000))\n",
    "# # remove back 8 seconds from non-mandookavani recordings\n",
    "# for i in range(0, len(data1)- len(mixedRecordings)):# - len(testingPaths) \n",
    "#     print(data3[i][0].shape)\n",
    "#     print(data3[i][1])\n",
    "# len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data3))\n",
    "mixedData = data3[len(data3) - len(mixedRecordings):]\n",
    "data3 = data3[:len(data3) - len(mixedRecordings)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time shift\n",
    "def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    sig_len = aud[1]\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    print(type(sig))\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "print(len(data3))\n",
    "len(mixedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1[0][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def noise_injection(data,noise_levels=(0, 0.5)):\n",
    "        sound, sr = data\n",
    "        print(type(sound[0][0]))\n",
    "        noise_level = np.random.uniform(*noise_levels)\n",
    "        noise = np.random.randn(len(sound[0]))\n",
    "        sound = np.array(sound)\n",
    "      \n",
    "        augmented_sound = sound + noise_level * noise\n",
    "        # Cast back to same data type\n",
    "        augmented_sound = augmented_sound.astype(type(sound[0]))\n",
    "        augmented_sound = torch.from_numpy(sound)\n",
    "        print(type(augmented_sound))\n",
    "        return (augmented_sound, sr)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_noise_injections(times, data, length):\n",
    "    for i in range(times):\n",
    "        for i in range(length):\n",
    "            data.append(noise_injection(data[i]))\n",
    "        print(\"hi\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_timeShifts(times, data, length):\n",
    "    for i in range(times):\n",
    "        for i in range(length):\n",
    "            data.append(time_shift(data[i], .5))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMES = 100\n",
    "#data3 = perform_timeShifts(1, data3, 15)\n",
    "#mixedData = perform_timeShifts(TIMES * 2, mixedData)\n",
    "\n",
    "len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #data3 = perform_noise_injections(1, data3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [1,2]\n",
    "sample = np.array(sample)\n",
    "sample = torch.from_numpy(sample)\n",
    "print(type(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mixedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mixedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(data3[4000][0][0][30000])\n",
    "# print(type(mixedData[0][0][0]))\n",
    "# print()\n",
    "# for i in range(15):\n",
    "    \n",
    "#     new_data = mixedData[i]\n",
    "#     pads = torch.zeros(new_data[1] * 22 - new_data[1] * 6)\n",
    "#     lastPadding = torch.zeros(new_data[1] * 18)\n",
    "#     #print(type(data))\n",
    "#     #print(type(new_data[0][0].numpy()))\n",
    "#   #  new_data[0][0] = new_data[0][0].numpy()\n",
    "#     #new_data[0][1] = np.array(new_data[0][1])\n",
    "#     #print(type(torch.from_numpy(new_data[0][0].numpy()[:new_data[1] * 6])))\n",
    "#     new_data[0][0] = torch.cat((torch.from_numpy(new_data[0][0].numpy()[:new_data[1] * 6]), pads))\n",
    "#     new_data[0][1] = torch.cat((torch.from_numpy(new_data[0][1].numpy()[:new_data[1] * 6]), pads))\n",
    "#    # print(mixedData.shape, new_data.shape)\n",
    "#     print(\"1\")\n",
    "#     mixedData.append(new_data)\n",
    "#     print(\"2\")\n",
    "#     new_data[0][0] = torch.cat((torch.from_numpy(new_data[0][0].numpy()[new_data[1] * 6:new_data[1] * 12]), pads))\n",
    "#     new_data[0][1] = torch.cat((torch.from_numpy(new_data[0][1].numpy()[new_data[1] * 6:new_data[1] * 12]), pads))\n",
    "#     mixedData.append(new_data)\n",
    "#     new_data[0][0] = torch.cat((torch.from_numpy(new_data[0][0].numpy()[new_data[1] * 12: new_data[1] * 18]), pads))\n",
    "#     new_data[0][1] = torch.cat((torch.from_numpy(new_data[0][1].numpy()[new_data[1] * 12: new_data[1] * 18]), pads))\n",
    "#     #print(data[0].shape)\n",
    "#     mixedData.append(new_data)\n",
    "#     new_data[0][0] = torch.cat((torch.from_numpy(new_data[0][0].numpy()[new_data[1] * 18:]), lastPadding))\n",
    "#     new_data[0][1] = torch.cat((torch.from_numpy(new_data[0][1].numpy()[new_data[1] * 18:]), lastPadding))\n",
    "#     mixedData.append(new_data)\n",
    "#     print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mixedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #mixedData = perform_timeShifts(2, mixedData, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixedData = perform_noise_injections(2, mixedData, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mixedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mixedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_labels = [21, 65, 7]\n",
    "# for i in range(TIMES * 4):\n",
    "#     testing_labels.extend([21,65,7])\n",
    "# # # creating testing labels\n",
    "# #MIXED RECORDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testingLabels = []\n",
    "# testingLabels.extend([0, 11, 15, 2, 2, 2, 25, 25, 30, 38, 39, 4, 41, 58, 9])\n",
    "# # for i in range(15):\n",
    "# #     testingLabels.extend([testingLabels[i],testingLabels[i], testingLabels[i], testingLabels[i] ])\n",
    "# for i in range(TIMES):\n",
    "#     testingLabels.extend([0, 11, 15, 2, 2, 2, 25, 25, 30, 38, 39, 4, 41, 58, 9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(testingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantLabel = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 3, 3, 4]\n",
    "#[0, 11, 15, 2, 25, 30, 38, 39, 4, 41, 58, 9, 2, 2, 25]\n",
    "labels = []\n",
    "labels.extend(instantLabel)\n",
    "# for i in range(2):\n",
    "#      labels.extend(instantLabel)\n",
    "    #print(len(labels))\n",
    "   \n",
    "   # labels.extend(testingLabels)\n",
    "# for i in range(len(labels)):\n",
    "#         labels[i] = int(labels[i])\n",
    "\n",
    "# for i in range(0, 4):\n",
    "#       print(labels[74])\n",
    "#       #0, 30, 51, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# labels1 = []\n",
    "# labels1.extend(updatedTestingLabels)\n",
    "# # for i in range(4):\n",
    "# #      labels1.extend(updatedTestingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(updatedTrainingLabels[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel spectrogram turning its hyperparameters\n",
    "def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melData = []\n",
    "tf_data = []\n",
    "data3.extend(mixedData)\n",
    "print(len(data3))\n",
    "\n",
    "for data in data3:\n",
    "    \n",
    "    specto = spectro_gram(data)\n",
    "    \n",
    "    melData.append(specto)\n",
    "   \n",
    "    # num_channels, num_mels, time\n",
    "print(\"hi\")\n",
    "for data in melData:\n",
    "    \n",
    "    #np_array = data.numpy()# DO I HAVE TO DO THIS LINE?\n",
    "    np_array = np.array(data)\n",
    "    tf_data.append(np_array)\n",
    "melData = np.array(melData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(spectro_augment(torch.from_numpy(tf_data[0])).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedData = tf_data[len(tf_data) - len(mixedData):]\n",
    "tf_data = tf_data[:len(tf_data) - len(mixedData)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialLen = len(tf_data)\n",
    "# for i in range(initialLen):   \n",
    " \n",
    "#     tf_data.append(spectro_augment(torch.from_numpy(tf_data[i])).numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels2 = labels\n",
    "# labels.extend(labels2)\n",
    "# #labels.extend(testingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # mixedData = list(mixedData)\n",
    "    # initialLen = len(mixedData)\n",
    "    # for i in range(initialLen): \n",
    "  \n",
    "    #     mixedData.append(spectro_augment(torch.from_numpy(mixedData[i])).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels2 = testingLabels\n",
    "# testingLabels.extend(labels2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(testingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tf_data:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_data = np.array(tf_data)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_data))\n",
    "print(.21\n",
    "       * len(tf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.array(labels)\n",
    "#testingLabels = np.array(testingLabels)\n",
    "\n",
    "# training_data = tf_data[:len(data3) - len(mixedData)]\n",
    "# testing_data = tf_data[len(data3) - len(mixedData):]\n",
    "# training_labels = labels\n",
    "\n",
    "#mixedData = tf_data[len(tf_data) - len(mixedData):]\n",
    "#tf_data = tf_data[: len(tf_data) - len(mixedData)]'\n",
    "# should i increase the amount of sample in testing set\n",
    "print(len(tf_data))\n",
    "print(len(labels))\n",
    "breakdown = int(.79\n",
    "                *len(tf_data))\n",
    "training_data = tf_data\n",
    "#testing_data = tf_data[breakdown:]\n",
    "training_labels = labels\n",
    "training_labels = np.array(training_labels)\n",
    "# testing_labels = labels[breakdown: ]# numpy arrays with numpy integer\n",
    "testing_data = mixedData\n",
    "testing_labels = testingLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use a pretrained model  AlexNet, Mobilenet, Xception\n",
    "# research neural networks \n",
    "\n",
    "print(type(training_data))\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_labels[9\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensor_to_img(spectrogram): \n",
    "#     plt.imshow(spectrogram[0],aspect='auto', origin='lower')\n",
    "#     plt.show();\n",
    "#     display(spectrogram.shape)\n",
    "# tensor_to_img(testing_data[138])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedData = np.array(mixedData)\n",
    "\n",
    "#testing_data = np.concatenate((testing_data, mixedData))\n",
    "#testing_labels = np.concatenate((testing_labels, testingLabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = np.array(testing_data)\n",
    "print(type(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.rollaxis(training_data,1,3)  \n",
    "training_data = np.rollaxis(training_data,3,2)  \n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = np.rollaxis(testing_data,1,3)  \n",
    "testing_data = np.rollaxis(testing_data,3,2)  \n",
    "print(testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "optimizer = keras.optimizers.Adam(lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=( 64, 469, 2) ))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "   # model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "    #model.add(layers.Conv2D(128, (1,1), activation='relu'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    #model.add(layers.Dropout(.25))\n",
    "#dropout.  1    \n",
    "    model.add(layers.Dense(12, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs worked well -> more 92.9\n",
    "# 40 epoc -> 95.6 accuracy\n",
    "# 60 -> 98.3 accuracy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(training_data, training_labels, epochs=60   , \n",
    "                    validation_data=(testing_data, testing_labels))\n",
    "\n",
    "# test 1 recording from test spceies\n",
    "# is mel spectrogram a case\n",
    "# try without augmentaiotn / small nn (40 % training accuracy 0-6% testing accuracy)\n",
    "# try with agmentaiton / small nn ( if jump is not big enough, mel spectrogarm not good neough ( small nn with 92% training accuracy, 66% testing accuracy), validiation accuracy failes to increase afrter accuracy of arround 70 percent training\n",
    "# try with augmentation / big nn (big nn with 93 % training accuracy, 66 accuracy)\n",
    "# svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(testing_data,  testing_labels, verbose=2)\n",
    "print(test_acc)\n",
    "# how to display confidence scores of each input??? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ (1 + math.exp(915.83))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(testing_data).round()\n",
    "#print(testing_data.shape)\n",
    "#print(prediction[11][1])\n",
    "updated_predictions = []\n",
    "maximum = -1\n",
    "for i in range(0, len(prediction)):\n",
    "    print(prediction[i][testing_labels[i]] * 100,  \"TestingLabel: \" + str(testing_labels[i]))\n",
    "    updated_predictions.append((prediction[i][testing_labels[i]] * 100,testing_labels[i]))\n",
    "    #for j in range(0, len(prediction[i])):\n",
    "       # if(prediction[i][j] > maximum):\n",
    "           # maximum = prediction[i][j]\n",
    "       # if prediction[i][j] < 0:\n",
    "           # prediction[i][j] = 0\n",
    "       # elif prediction [i][j] < -32:\n",
    "          #  prediction[i][j] = 0\n",
    "       # else:\n",
    "           # prediction[i][j] = sigmoid(prediction[i][j])\n",
    "\n",
    "#print(testing_labels[0])\n",
    "print(prediction[2])\n",
    "#print(maximum)\n",
    "#print(\"prediction shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(prediction)\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "print(prediction)\n",
    "print(metrics.f1_score(prediction, testing_labels, average='micro', pos_label='positive'))\n",
    "print(metrics.precision_score(prediction, testing_labels, average='micro', pos_label='positive'))\n",
    "print(metrics.recall_score(prediction, testing_labels, average='micro', pos_label='positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracies_for_each_class(predictions):\n",
    "    new_predictions  = []\n",
    "    for i in range(NUM_CLASSES):\n",
    "        num = 0\n",
    "        counter =0\n",
    "        for accuracy in predictions:\n",
    "            if (accuracy[1] == i):\n",
    "                num += accuracy[0]\n",
    "                counter  = counter + 1\n",
    "        if counter != 0:\n",
    "            average = num / counter\n",
    "        else:\n",
    "            average = 0\n",
    "        new_predictions.append((average, i))\n",
    "    return new_predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_predictions = accuracies_for_each_class(updated_predictions)\n",
    "print(prediction[15])\n",
    "print(average_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n",
    "print(len(testing_labels))\n",
    "print(len(class_names))\n",
    "print(testing_labels.shape)\n",
    "print(prediction.shape)\n",
    "prediction = np.argmax(prediction, axis = 1)\n",
    "print(prediction.shape)\n",
    "cm = confusion_matrix(testing_labels, prediction)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(200,200))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = ConfusionMatrixDisplay(cm, )\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "cmd.plot(ax=ax, )\n",
    "plt.xticks(rotation=270)\n",
    "plt.show()\n",
    "#0, 15, 30, 39, 4, 41, 58, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cmd/np.sum(cmd), annot=True, \n",
    "            fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = {-1}\n",
    "# for predict in prediction:\n",
    "#     counter = 0\n",
    "#     for pre in predict:\n",
    "#         if(pre  >= .1):\n",
    "#             sample.add(counter)\n",
    "            \n",
    "#             counter = 0\n",
    "#             break\n",
    "#         counter = counter + 1\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "#     if ax is None:\n",
    "#         _, ax = plt.subplots(1, 1)\n",
    "#     if title is not None:\n",
    "#         ax.set_title(title)\n",
    "#     ax.set_ylabel(ylabel)\n",
    "#     ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "#     plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 1)\n",
    "# #a = librosa.power_to_db(training_data[0])\n",
    "# plot_spectrogram(training_data[0],title=\"spectrogram\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# print(np.array(data1[0][0].shape))\n",
    "# sample = np.array(data1[0][0][:1])\n",
    "# sample2 = sample\n",
    "# print(sample2.shape)\n",
    "# spect = librosa.feature.melspectrogram(y=np.array(data1[0][0][:1]), sr=48000, n_mels=128,\n",
    "#                                     fmax=8000)\n",
    "# print(spect.shape)\n",
    "# S_dB = librosa.power_to_db(spect, ref=np.max)\n",
    "# print(S_dB.shape)\n",
    "# img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "#                          y_axis='mel', sr=48000,\n",
    "#                          fmax=8000, ax=ax)\n",
    "# fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "# ax.set(title='Mel-frequency spectrogram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
